{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12106805,"sourceType":"datasetVersion","datasetId":7316581}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:02:43.834075Z","iopub.execute_input":"2025-06-09T20:02:43.834639Z","iopub.status.idle":"2025-06-09T20:02:43.840756Z","shell.execute_reply.started":"2025-06-09T20:02:43.834614Z","shell.execute_reply":"2025-06-09T20:02:43.840107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/facebookresearch/esm.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:02:43.847204Z","iopub.execute_input":"2025-06-09T20:02:43.847471Z","iopub.status.idle":"2025-06-09T20:02:52.833528Z","shell.execute_reply.started":"2025-06-09T20:02:43.847456Z","shell.execute_reply":"2025-06-09T20:02:52.832548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom collections import defaultdict\nimport random\n\nseed = 7\n\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\nimport esm\n\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import functional as F\n\nfrom transformers import Trainer, TrainingArguments, EvalPrediction\nfrom datasets import load_dataset\n\n# from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom scipy import stats\nfrom functools import partial\nimport pandas as pd\nfrom tqdm.auto import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:02:52.835425Z","iopub.execute_input":"2025-06-09T20:02:52.835652Z","iopub.status.idle":"2025-06-09T20:02:52.844659Z","shell.execute_reply.started":"2025-06-09T20:02:52.835632Z","shell.execute_reply":"2025-06-09T20:02:52.843880Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Model and Data","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Available device:', device)\ndef check_len(some_list):\n    for i in some_list:\n        print(len(i))\n\n\ndef get_num_params(model):\n    return sum(p.numel() for p in model.parameters())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:02:52.845386Z","iopub.execute_input":"2025-06-09T20:02:52.845581Z","iopub.status.idle":"2025-06-09T20:02:52.862824Z","shell.execute_reply.started":"2025-06-09T20:02:52.845566Z","shell.execute_reply":"2025-06-09T20:02:52.862233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, tokenizer = esm.pretrained.esm2_t33_650M_UR50D()\nbatch_converter = tokenizer.get_batch_converter()\nmodel.eval()\nmodel.to(device=device)\nprint(f\"Number of parameters:\", get_num_params(model))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:02:52.864440Z","iopub.execute_input":"2025-06-09T20:02:52.864609Z","iopub.status.idle":"2025-06-09T20:03:01.484430Z","shell.execute_reply.started":"2025-06-09T20:02:52.864596Z","shell.execute_reply":"2025-06-09T20:03:01.482821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.all_special_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.484770Z","iopub.status.idle":"2025-06-09T20:03:01.484985Z","shell.execute_reply.started":"2025-06-09T20:03:01.484882Z","shell.execute_reply":"2025-06-09T20:03:01.484892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_sequence_id(index_name):\n    \"\"\"\n    Extract sequence ID from index like 'sequence_0_window_1'\n    \n    Parameters:\n    -----------\n    index_name : str\n        Index name in format 'sequence_[id]_window_[window_id]'\n    \n    Returns:\n    --------\n    str\n        The sequence ID (e.g., '0' from 'sequence_0_window_1')\n    \"\"\"\n    # Use regex to extract sequence ID\n    match = re.search(r'sequence_(\\d+)_window_\\d+', index_name)\n    if match:\n        return match.group(1)\n    else:\n        # Alternative pattern matching if format is different\n        parts = index_name.split('_')\n        if len(parts) >= 4 and parts[0] == 'sequence':\n            return parts[1]\n    return None\n\ndef sequence_based_split(df, val_size=0.2, random_state=42):\n    \"\"\"\n    Split dataframe based on sequence IDs to prevent data leakage.\n    All windows from the same sequence will be in the same split.\n    \n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        DataFrame with index containing sequence and window information\n    test_size : float\n        Proportion of sequences for test set\n    val_size : float\n        Proportion of remaining sequences for validation set\n    random_state : int\n        Random seed for reproducibility\n    \n    Returns:\n    --------\n    tuple\n        (train_df, val_df, test_df, split_info)\n    \"\"\"\n    # Extract sequence IDs from index\n    sequence_ids = []\n    for idx in df['id']:\n        seq_id = extract_sequence_id(str.lower(idx))\n        if seq_id is not None:\n            sequence_ids.append(seq_id)\n        else:\n            raise ValueError(f\"Could not extract sequence ID from index: {idx}\")\n    \n    # Add sequence IDs to dataframe (temporary)\n    df_temp = df.copy()\n    df_temp['sequence_id'] = sequence_ids\n    \n    # Get unique sequence IDs\n    unique_sequences = list(set(sequence_ids))\n    print(f\"Total unique sequences: {len(unique_sequences)}\")\n    \n    # Count windows per sequence\n    sequence_counts = defaultdict(int)\n    for seq_id in sequence_ids:\n        sequence_counts[seq_id] += 1\n    \n    print(f\"Windows per sequence: min={min(sequence_counts.values())}, \"\n          f\"max={max(sequence_counts.values())}, \"\n          f\"mean={np.mean(list(sequence_counts.values())):.1f}\")\n    \n    # First split: separate test sequences\n    train_sequences, val_sequences = train_test_split(\n        unique_sequences, \n        test_size=val_size, \n        random_state=random_state\n    )\n    \n    # Create splits based on sequence membership\n    train_df = df_temp[df_temp['sequence_id'].isin(train_sequences)].drop('sequence_id', axis=1)\n    val_df = df_temp[df_temp['sequence_id'].isin(val_sequences)].drop('sequence_id', axis=1)\n    \n    # Create split information\n    split_info = {\n        'train_sequences': sorted(train_sequences),\n        'val_sequences': sorted(val_sequences),\n        'train_windows': len(train_df),\n        'val_windows': len(val_df),\n        'total_windows': len(df)\n    }\n    \n    print(f\"Split results:\")\n    print(f\"  Train: {len(train_sequences)} sequences, {len(train_df)} windows\")\n    print(f\"  Val:   {len(val_sequences)} sequences, {len(val_df)} windows\") \n    \n    return train_df, val_df, split_info\n\ndef verify_no_leakage(train_df, val_df):\n    \"\"\"\n    Verify that there's no sequence leakage between splits.\n    \n    Parameters:\n    -----------\n    train_df, val_df, test_df : pandas.DataFrame\n        The split dataframes\n    \n    Returns:\n    --------\n    bool\n        True if no leakage detected, False otherwise\n    \"\"\"\n    # Extract sequence IDs from each split\n    train_seqs = set([extract_sequence_id(str(idx)) for idx in train_df.index])\n    val_seqs = set([extract_sequence_id(str(idx)) for idx in val_df.index])\n    \n    # Check for overlaps\n    train_val_overlap = list(train_seqs.intersection(val_seqs))\n    \n    if len(train_val_overlap) > 1 or train_val_overlap[0] is not None:\n        print(\"❌ DATA LEAKAGE DETECTED!\")\n        if train_val_overlap:\n            print(f\"  Train-Val overlap: {train_val_overlap}\")\n        return False\n    else:\n        print(\"✅ No data leakage detected. All sequences are properly separated.\")\n        return True\n\n# Example usage and demonstration\ndef split_dataset_process(df):\n    \"\"\"Demonstrate the sequence-based splitting functionality\"\"\"\n    \n    # Test regular sequence-based split\n    print(\"=== Regular Sequence-Based Split Process ===\")\n    train_df, val_df, split_info = sequence_based_split(\n        df, val_size=0.25, random_state=42\n    )\n    \n    print()\n    verify_no_leakage(train_df, val_df)\n    print()\n\n    \n    return train_df, val_df, split_info\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.485994Z","iopub.status.idle":"2025-06-09T20:03:01.486284Z","shell.execute_reply.started":"2025-06-09T20:03:01.486129Z","shell.execute_reply":"2025-06-09T20:03:01.486140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df =  pd.read_csv('/kaggle/input/preprocessed-v4/PS4_Train_108cut_ESM.csv')\ntrain_df['len_seq_aa'] = train_df['aa_sequence'].apply(lambda x : len(str(x)))\ntrain_df['len_ssp8'] = train_df['ssp_sequence'].apply(lambda x : len(str(x)))\nprint(\"=== Training PS4 Dataset ===\")\nprint(train_df.info())\nval_df =  pd.read_csv('/kaggle/input/preprocessed-v4/PS4_Val_108cut_ESM.csv')\nval_df['len_seq_aa'] = val_df['aa_sequence'].apply(lambda x : len(str(x)))\nval_df['len_ssp8'] = val_df['ssp_sequence'].apply(lambda x : len(str(x)))\nprint(\"=== Validation PS4 Dataset ===\")\nprint(val_df.info())\n\ndf_cb433 = pd.read_csv('/kaggle/input/preprocessed-v4/CB433_108cut_ESM.csv')\n# df_cb433 = pd.read_csv('/kaggle/input/preprocessed-v4/CB433_108cut.csv')\nprint(\"=== Test CB433 Dataset ===\")\nprint(df_cb433.info())\ndf_cb433['len_seq_aa'] = df_cb433['aa_sequence'].apply(lambda x : len(str(x)))\ndf_cb433['len_ssp8'] = df_cb433['ssp_sequence'].apply(lambda x : len(str(x)))\nprint(df_cb433.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.487571Z","iopub.status.idle":"2025-06-09T20:03:01.487862Z","shell.execute_reply.started":"2025-06-09T20:03:01.487698Z","shell.execute_reply":"2025-06-09T20:03:01.487715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def wrong_eos(df):\n    # --- Step 1: Fix sequences ending with '<', '/', 's' ---\n    df = df.copy()  # Avoid SettingWithCopyWarning\n    mask = df['aa_sequence'].str[-1].isin(['<', '/', 's'])\n\n    # Apply replacement based on last character\n    ends_with_s = mask & (df['aa_sequence'].str[-1] == 's')\n    ends_with_slash = mask & (df['aa_sequence'].str[-1] == '/')\n    ends_with_lt = mask & (df['aa_sequence'].str[-1] == '<')\n\n    # Use .loc to safely assign new values\n    df.loc[ends_with_s, 'aa_sequence'] = df.loc[ends_with_s, 'aa_sequence'].str[:-3] + '<eos>'\n    df.loc[ends_with_slash, 'aa_sequence'] = df.loc[ends_with_slash, 'aa_sequence'].str[:-2] + '<eos>'\n    df.loc[ends_with_lt, 'aa_sequence'] = df.loc[ends_with_lt, 'aa_sequence'].str[:-1] + '<eos>'\n\n    # --- Step 2: Drop rows where ssp_sequence is all 'X' ---\n    mask_ssp_all_x = df['ssp_sequence'].str.fullmatch(r'^X+$')\n    df = df[~mask_ssp_all_x].reset_index(drop=True)\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.488639Z","iopub.status.idle":"2025-06-09T20:03:01.488852Z","shell.execute_reply.started":"2025-06-09T20:03:01.488752Z","shell.execute_reply":"2025-06-09T20:03:01.488761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# replace J as unknown\ntrain_df['aa_sequence'] = train_df['aa_sequence'].str.replace('J', 'X')\nval_df['aa_sequence'] = val_df['aa_sequence'].str.replace('J', 'X')\ndf_cb433['aa_sequence']= df_cb433['aa_sequence'].str.replace('J', 'X')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.490068Z","iopub.status.idle":"2025-06-09T20:03:01.490340Z","shell.execute_reply.started":"2025-06-09T20:03:01.490176Z","shell.execute_reply":"2025-06-09T20:03:01.490185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df[train_df['aa_sequence'].str.contains('J')]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.491105Z","iopub.status.idle":"2025-06-09T20:03:01.491410Z","shell.execute_reply.started":"2025-06-09T20:03:01.491240Z","shell.execute_reply":"2025-06-09T20:03:01.491251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = wrong_eos(train_df)\nval_df = wrong_eos(val_df)\ndf_cb433 = wrong_eos(df_cb433)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.493298Z","iopub.status.idle":"2025-06-09T20:03:01.493582Z","shell.execute_reply.started":"2025-06-09T20:03:01.493421Z","shell.execute_reply":"2025-06-09T20:03:01.493434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_wrong_eos(df):\n    result_indices = df[(df['aa_sequence'].str[-1].isin(['<', '/', 's'])) | \n                        (df['ssp_sequence'].str.fullmatch(r'X+'))].index.tolist()\n    return result_indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.494573Z","iopub.status.idle":"2025-06-09T20:03:01.494838Z","shell.execute_reply.started":"2025-06-09T20:03:01.494710Z","shell.execute_reply":"2025-06-09T20:03:01.494729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_wrong_eos(train_df), check_wrong_eos(val_df), check_wrong_eos(df_cb433)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.496239Z","iopub.status.idle":"2025-06-09T20:03:01.496491Z","shell.execute_reply.started":"2025-06-09T20:03:01.496383Z","shell.execute_reply":"2025-06-09T20:03:01.496397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# val_df = pd.read_csv('/kaggle/input/preprocessed-v4/Val_PS4.csv')\ndfs = [\ntrain_df,\n       val_df,\ndf_cb433]\n# dfs = [val_df, df_cb433]\n\n# train_df.to_csv('Train_PS4.csv')\n# val_df.to_csv('Val_PS4.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.497189Z","iopub.status.idle":"2025-06-09T20:03:01.497483Z","shell.execute_reply.started":"2025-06-09T20:03:01.497359Z","shell.execute_reply":"2025-06-09T20:03:01.497376Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Processing Dataset","metadata":{}},{"cell_type":"code","source":"def tokenize_sequence(sequence: str):\n        # Define the special tokens\n        special_tokens = {\"<cls>\", \"<eos>\", \"<pad>\"} #esm\n        # special_tokens = [\"</s>\", \"<pad>\"]\n        # special_tokens = {\"[CLS]\", \"[PAD]\"}\n        result = []\n        i = 0\n        n = len(sequence)\n        \n        while i < n:\n            # Check if the current position starts a special token\n            if sequence[i:i+4] in special_tokens[0]:  # Handle </s>\n                result.append(sequence[i:i+4])\n                i += 4  # Skip the length of the special token\n            elif sequence[i:i+5] in special_tokens[1]:  # Handle <pad>\n                result.append(sequence[i:i+5])\n                i += 5  # Skip the length of the special token\n            else:\n                # Add individual characters\n                result.append(sequence[i])\n                i += 1\n    \n        return result\n    \ndef preprocessed(df_col):\n    processed_sequences = []\n    for seq in df_col:\n        # prep_seq = list(seq)\n        # prep_seq = tokenize_sequence(seq) #only for Ankh\n        processed_sequences.append(prep_seq)\n    return processed_sequences\n    \ndef preprocess_dataset(sequences, labels, \n                       # disorder, \n                       max_length=None, st = True):\n    \n    # if st:\n    #     sequences = [\"\".join(str(seq).split()) for seq in sequences]\n    # else:\n    #     sequences = preprocessed(sequences)\n    \n    if max_length is None:\n        max_length = len(max(sequences, key=lambda x: len(x)))\n\n    seqs = [list(seq)[:max_length] for seq in sequences]\n    \n    labels = [\"\".join(str(label).split()) for label in labels]\n    labels = [list(label)[:max_length] for label in labels]\n    \n    # disorder = [\" \".join(disorder.split()) for disorder in disorder]\n    # disorder = [disorder.split()[:max_length] for disorder in disorder]\n    \n    assert len(seqs) == len(labels)\n    return seqs, labels\n\n\ndef embed_dataset(model, tokenizer, sequences, \n                   device, max_length=108):\n    \"\"\"\n    Embed sequences using a pre-trained model and adjust the attention mask to only include \n    actual content (set to 0 after the last non-padding token).\n    \n    Args:\n        model: Pre-trained model for embedding the sequences\n        sequences: List of sequences to embed\n        tokenizer: Tokenizer to convert sequences to token IDs\n        device: Device to run the model on (e.g., 'cuda' or 'cpu')\n        max_length: Maximum sequence length for padding/truncation\n        \n    Returns:\n        Tuple of (inputs_id, inputs_embedding, adjusted_attention_masks)\n    \"\"\"\n    inputs_embedding = []\n    # inputs_id = []\n    # adjusted_attention_masks = []\n\n    #tokenize input\n    batch_labels, batch_strs, batch_tokens = batch_converter(sequences)\n    batch_tokens = post_filter_tokens(batch_tokens, max_length).to(device)\n    \n    with torch.no_grad():\n        # for sample in tqdm(sequences):\n            \n            # Generate embeddings\n            embedding = model(batch_tokens, repr_layers = [0], return_contacts = False)['representations'][0].to(device)\n            \n            inputs_embedding.append(embedding)\n            # inputs_id.append(ids)\n            # adjusted_attention_masks.append(adjusted_mask)\n    \n    return inputs_embedding\n\n\ndef embed_dataset(model, tokenizer, sequences, device, max_length=108, batch_size=10):\n    \"\"\"\n    Embed sequences in batches for memory efficiency.\n    \n    Args:\n        model: Pre-trained model for embedding the sequences\n        tokenizer: Tokenizer to convert sequences to token IDs\n        sequences: List of sequences to embed\n        device: Device to run the model on ('cuda' or 'cpu')\n        max_length: Max length for truncation/padding\n        batch_size: Number of sequences per batch (reduce if OOM)\n        \n    Returns:\n        List of embeddings (on CPU) for all sequences\n    \"\"\"\n    \n    all_embeddings = []\n\n    # Process in batches\n    for i in range(0, len(sequences), batch_size):\n        batch_seq = sequences[i:i + batch_size]\n        \n        # Tokenize batch\n        batch_labels, batch_strs, batch_tokens = batch_converter(batch_seq)\n        batch_tokens = post_filter_tokens(batch_tokens, max_length).to(device)\n\n        with torch.no_grad():\n            # Generate embeddings\n            outputs = model(batch_tokens, repr_layers=[0], return_contacts=False)\n            embeddings = outputs['representations'][0]  # Shape: [B, L, D]\n\n        # Move embeddings to CPU and clear GPU memory\n        all_embeddings.extend(embeddings.cpu())\n        del batch_tokens, embeddings, outputs\n        torch.cuda.empty_cache()\n\n    return all_embeddings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.498637Z","iopub.status.idle":"2025-06-09T20:03:01.498915Z","shell.execute_reply.started":"2025-06-09T20:03:01.498800Z","shell.execute_reply":"2025-06-09T20:03:01.498813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def post_filter_tokens(batch_tokens, max_length):\n    \"\"\"\n    Apply post-filtering rules to batch tokens:\n    1. If there are 2 zeros in front, remove one (keep only one zero)\n    2. If there is 1 zero in front, remove it\n    3. If there are more than 1 occurrence of 2s, remove the farthest 2s and everything after\n    4. Check if there are 1s, ensure 2s come before them\n    5. Ensure all sequences have the same length by padding\n    \"\"\"\n    filtered_batch = []\n    \n    for sequence in batch_tokens:\n        seq = sequence.clone()\n        seq_list = seq.tolist()\n        \n        # Rule 1 & 2: Handle leading zeros\n        # while len(seq_list) > 1 and seq_list[0] == 0 and seq_list[1] == 0:\n        #     # If there are 2+ zeros at start, remove one\n        #     seq_list.pop(0)\n        \n        if len(seq_list) > 0 and seq_list[0] == 0:\n            # If there's still 1 zero at start, remove it\n            seq_list.pop(0)\n        \n        # Rule 3: Handle multiple 2s - keep only up to the first occurrence of 2\n        two_indices = [i for i, x in enumerate(seq_list) if x == 2]\n        if len(two_indices) == 1:\n            first_two_idx = two_indices[0]\n            seq_list = seq_list[:first_two_idx]\n        elif len(two_indices) > 1:\n            # Remove everything from the second occurrence of 2 onwards\n            first_two_idx = two_indices[0]\n            second_two_idx = two_indices[1]\n            seq_list = seq_list[:second_two_idx]\n        \n        # Rule 4: Ensure 2s come before 1s\n        ones_indices = [i for i, x in enumerate(seq_list) if x == 1]\n        twos_indices = [i for i, x in enumerate(seq_list) if x == 2]\n        \n        if ones_indices and twos_indices:\n            first_one_idx = min(ones_indices)\n            last_two_idx = max(twos_indices)\n            \n            # If any 1 appears before the last 2, we need to fix the order\n            if first_one_idx < last_two_idx:\n                # Separate 2s, other values, and 1s\n                twos = [x for x in seq_list if x == 2]\n                ones = [x for x in seq_list if x == 1]\n                others = [x for x in seq_list if x != 1 and x != 2]\n                \n                # Reconstruct: others first, then 2s, then 1s\n                seq_list = others + twos + ones\n        \n        filtered_batch.append(seq_list)\n    \n    # Rule 5: Ensure all sequences have the same length\n    if filtered_batch:\n        aug_seq = []\n        # Pad sequences to max_length with a padding token (using 1 as pad token)\n        for i, seq in enumerate(filtered_batch):\n            seq = aug_seq + seq\n            if len(seq) < max_length:\n                seq.extend([1] * (max_length - len(seq)))\n            elif len(seq) > max_length:\n                if seq[max_length] != 1:\n                    aug_seq = seq[max_length:]\n                seq = seq[:max_length]\n\n            filtered_batch[i] = seq\n    \n    # Convert back to tensor\n    if filtered_batch:\n        return torch.tensor(filtered_batch, dtype=batch_tokens.dtype)\n    else:\n        return torch.empty((0, 0), dtype=batch_tokens.dtype)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.500151Z","iopub.status.idle":"2025-06-09T20:03:01.500559Z","shell.execute_reply.started":"2025-06-09T20:03:01.500392Z","shell.execute_reply":"2025-06-09T20:03:01.500407Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Processing Labels","metadata":{}},{"cell_type":"code","source":"# Consider each label as a tag for each token\n# training_sequences, training_labels = preprocess_dataset(df_pad['aa_sequence'], \n#                                                         df_pad['ssp_sequence'], \n#                                                          st = False\n#                                                         )\n# unique_tags = set(tag for doc in training_labels for tag in doc)\n# unique_tags.remove('X')\n# tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n# id2tag = {id: tag for tag, id in tag2id.items()}\n# del training_sequences, training_labels\n# print(tag2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.501328Z","iopub.status.idle":"2025-06-09T20:03:01.501631Z","shell.execute_reply.started":"2025-06-09T20:03:01.501474Z","shell.execute_reply":"2025-06-09T20:03:01.501487Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"{'T': 0, 'S': 1, 'G': 2, 'P': 3, 'L': 4, 'E': 5, 'B': 6, 'H': 7, 'I': 8}","metadata":{}},{"cell_type":"code","source":"\ndef encode_tags(labels, max_length=108, padding_value = -100):\n    \"\"\"\n    Encode tags to IDs and pad to a fixed length.\n    \n    Args:\n        labels: List of lists, where each inner list contains tags for a document\n        max_length: Maximum length for padding (default: 108)\n    \n    Returns:\n        List of lists with tag IDs and padding\n    \"\"\"\n    encoded_labels = []\n    \n    for doc in labels:\n        # Convert tags to IDs, skip unknown tags\n        doc_ids = [tag2id.get(tag, padding_value) for tag in doc]  # Use -100 for unknown tags\n        \n        # Truncate if longer than max_length\n        if len(doc_ids) > max_length:\n            doc_ids = doc_ids[:max_length]\n        # Add padding if shorter than max_length\n        else: \n            padding = [padding_value] * (max_length - len(doc_ids))\n            doc_ids = doc_ids + padding\n            \n        encoded_labels.append(doc_ids)\n    \n    return encoded_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.502798Z","iopub.status.idle":"2025-06-09T20:03:01.503408Z","shell.execute_reply.started":"2025-06-09T20:03:01.503229Z","shell.execute_reply":"2025-06-09T20:03:01.503246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef tensor_to_numpy(tensor):\n    \"\"\"Safely convert tensor to numpy array\"\"\"\n    # Check if it's a PyTorch tensor\n    if torch.is_tensor(tensor):\n        return tensor.detach().cpu().numpy()\n    elif isinstance(tensor, np.ndarray):\n        return tensor\n    elif isinstance(tensor, list):\n        # Handle list of tensors\n        converted_list = []\n        for item in tensor:\n            if torch.is_tensor(item):\n                converted_list.append(item.detach().cpu().numpy())\n            else:\n                converted_list.append(item)\n        return np.array(converted_list)\n    else:\n        # For other types, convert to numpy array safely\n        try:\n            return np.array(tensor)\n        except Exception as e:\n            print(f\"Error converting {type(tensor)} to numpy: {e}\")\n            raise\n\ndef memory_efficient_processing(model, batch_converter, df, tag2id, label, chunk_size=1000):\n    total_chunks = (len(df) - 1) // chunk_size + 1\n    results = []\n    \n    with tqdm(total=total_chunks, desc=\"Processing chunks\") as pbar:\n        input_column_name = 'aa_sequence'\n        labels_column_name = 'ssp_sequence'\n        padding_value = -100\n        # Create output directory\n        output_dir = './embedding_Ankh_result/'\n        os.makedirs(output_dir, exist_ok=True)\n\n        for i in range(0, len(df), chunk_size):\n            # Get chunk\n            chunk_num = i // chunk_size\n            chunk = df.iloc[i:i + chunk_size].copy()\n            print(f\"Processing chunk {chunk_num + 1}/{(len(df)-1)//chunk_size + 1}\")\n\n            #preprocessing dataset\n            training_sequences = list(zip(chunk['id'], chunk[input_column_name]))\n            training_labels = chunk[labels_column_name]\n\n            # training_sequences, training_labels = preprocess_dataset(training_sequences, \n            #                                             training_labels, \n            #                                              st = False\n            #                                             )\n\n             # Clear memory\n            del chunk\n            # Process embeddings\n            training_embeddings = embed_dataset(model, batch_converter, training_sequences,\n                                                device)\n             # Clear memory\n            del training_sequences\n            # Save to NPZ\n            npz_path = os.path.join(output_dir, f'{label}_sequence_chunk_{chunk_num:04d}.npz')\n            \n            # Save embeddings and any metadata\n            np.savez_compressed(\n                npz_path,\n                embeddings=tensor_to_numpy(training_embeddings),\n                # indices=chunk.index.values,  # Original indices\n                chunk_start=i,\n                chunk_end=min(i + chunk_size, len(df))\n            )\n            del training_embeddings\n            \n            #process label encoding\n            train_labels_encodings = torch.tensor(encode_tags(training_labels, padding_value = padding_value))\n            train_tensor_enc = F.one_hot(train_labels_encodings.clamp(min=0), num_classes=9)\n            # Replace one-hot vectors for padding values with all zeros\n            # This creates a mask where True indicates padding positions\n            padding_mask = (train_labels_encodings == padding_value).unsqueeze(-1)\n            # Zero out the one-hot vectors at padding positions\n            train_tensor_enc = train_tensor_enc.masked_fill(padding_mask, 0)\n            print(f\"Final shape: {train_tensor_enc.shape}\") \n\n            del training_labels\n\n            # Save to NPZ\n            npz_path = os.path.join(output_dir, f'{label}_labels_chunk_{chunk_num:04d}.npz')\n            \n            # Save embeddings and any metadata\n            np.savez_compressed(\n                npz_path,\n                embeddings=tensor_to_numpy(train_tensor_enc),\n                # indices=chunk.index.values,  # Original indices\n                chunk_start=i,\n                chunk_end=min(i + chunk_size, len(df))\n            )\n            # Clear memory\n            del train_labels_encodings, train_tensor_enc, padding_mask\n            \n            # Update progress\n            pbar.update(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.504474Z","iopub.status.idle":"2025-06-09T20:03:01.504725Z","shell.execute_reply.started":"2025-06-09T20:03:01.504597Z","shell.execute_reply":"2025-06-09T20:03:01.504607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tag2id = {'S': 0, 'H': 1, 'B': 2, 'I': 3, 'L': 4, 'P': 5, 'G': 6, 'E': 7, 'T': 8}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.506291Z","iopub.status.idle":"2025-06-09T20:03:01.506583Z","shell.execute_reply.started":"2025-06-09T20:03:01.506467Z","shell.execute_reply":"2025-06-09T20:03:01.506482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_dfs = ['train', 'val', 'test']\n# label_dfs = ['val']\nfor i in range(len(dfs)):\n    memory_efficient_processing(model, batch_converter, dfs[i], tag2id, label = label_dfs[i], chunk_size=5000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.507563Z","iopub.status.idle":"2025-06-09T20:03:01.507816Z","shell.execute_reply.started":"2025-06-09T20:03:01.507706Z","shell.execute_reply":"2025-06-09T20:03:01.507716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def merge_npz_files(input_pattern, output_file, merge_arrays=True):\n    \"\"\"\n    Merge multiple NPZ files into one\n    \n    Args:\n        input_pattern: Pattern to match files (e.g., \"train_*.npz\" or list of filenames)\n        output_file: Output NPZ filename\n        merge_arrays: If True, concatenate arrays with same keys. If False, keep separate with indices\n    \"\"\"\n    # Get list of files\n    if isinstance(input_pattern, str):\n        npz_files = sorted(glob.glob(input_pattern))\n    else:\n        npz_files = input_pattern\n    \n    print(f\"Found {len(npz_files)} NPZ files to merge:\")\n    for f in npz_files:\n        print(f\"  - {f}\")\n    \n    if not npz_files:\n        print(\"No NPZ files found!\")\n        return\n    \n    # Load first file to get structure\n    with np.load(npz_files[0]) as first_file:\n        array_names = list(first_file.keys())\n        print(f\"Array keys found: {array_names}\")\n    \n    if merge_arrays:\n        # Method 1: Concatenate arrays with same keys\n        merged_data = {}\n        \n        for array_name in array_names:\n            arrays_to_merge = []\n            \n            for npz_file in npz_files:\n                with np.load(npz_file) as data:\n                    if array_name in data:\n                        arrays_to_merge.append(data[array_name])\n                        print(f\"Loading {array_name} from {npz_file}: shape {data[array_name].shape}\")\n            \n            if arrays_to_merge:\n                # Concatenate along first axis (assuming batch dimension)\n                merged_array = np.concatenate(arrays_to_merge, axis=0)\n                merged_data[array_name] = merged_array\n                print(f\"Merged {array_name}: final shape {merged_array.shape}\")\n        \n        # Save merged data\n        np.savez_compressed(output_file, **merged_data)\n        print(f\"Saved merged data to {output_file}\")\n        \n    else:\n        # Method 2: Keep arrays separate with file indices\n        merged_data = {}\n        \n        for i, npz_file in enumerate(npz_files):\n            with np.load(npz_file) as data:\n                for array_name in data.keys():\n                    key = f\"{array_name}_file_{i}\"\n                    merged_data[key] = data[array_name]\n                    print(f\"Added {key}: shape {data[array_name].shape}\")\n        \n        # Save with indexed keys\n        np.savez_compressed(output_file, **merged_data)\n        print(f\"Saved indexed data to {output_file}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.508786Z","iopub.status.idle":"2025-06-09T20:03:01.509066Z","shell.execute_reply.started":"2025-06-09T20:03:01.508907Z","shell.execute_reply":"2025-06-09T20:03:01.508917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# output_dir = '/kaggle/working/embedding_Ankh_result/'\n# \"\"\"Merge specific files by name\"\"\"\n# files_to_merge = [\n#         # output_dir + \"train_labels_chunk_0000.npz\",\n#         # output_dir + \"train_labels_chunk_0001.npz\", \n#         output_dir + \"train_sequence_chunk_0000.npz\",\n#         output_dir + \"train_sequence_chunk_0001.npz\"\n#     ]\n\n# merge_npz_files(files_to_merge, \"merged_specific.npz\", merge_arrays=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:03:01.509988Z","iopub.status.idle":"2025-06-09T20:03:01.510288Z","shell.execute_reply.started":"2025-06-09T20:03:01.510100Z","shell.execute_reply":"2025-06-09T20:03:01.510114Z"}},"outputs":[],"execution_count":null}]}