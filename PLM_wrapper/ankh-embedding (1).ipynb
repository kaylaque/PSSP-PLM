{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12271810,"sourceType":"datasetVersion","datasetId":7733372}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:16:34.247850Z","iopub.execute_input":"2025-06-24T17:16:34.248174Z","iopub.status.idle":"2025-06-24T17:16:34.259992Z","shell.execute_reply.started":"2025-06-24T17:16:34.248142Z","shell.execute_reply":"2025-06-24T17:16:34.259038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q ankh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:16:34.261629Z","iopub.execute_input":"2025-06-24T17:16:34.261934Z","iopub.status.idle":"2025-06-24T17:16:38.808212Z","shell.execute_reply.started":"2025-06-24T17:16:34.261914Z","shell.execute_reply":"2025-06-24T17:16:38.806655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom collections import defaultdict\nimport random\n\nseed = 7\n\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\nimport ankh\n\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import functional as F\n\nfrom transformers import Trainer, TrainingArguments, EvalPrediction\nfrom datasets import load_dataset\n\n# from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom scipy import stats\nfrom functools import partial\nimport pandas as pd\nfrom tqdm.auto import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:16:38.809830Z","iopub.execute_input":"2025-06-24T17:16:38.810136Z","iopub.status.idle":"2025-06-24T17:16:38.827055Z","shell.execute_reply.started":"2025-06-24T17:16:38.810108Z","shell.execute_reply":"2025-06-24T17:16:38.826040Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Model and Data","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Available device:', device)\ndef check_len(some_list):\n    for i in some_list:\n        print(len(i))\n\n\ndef get_num_params(model):\n    return sum(p.numel() for p in model.parameters())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:16:38.828362Z","iopub.execute_input":"2025-06-24T17:16:38.828754Z","iopub.status.idle":"2025-06-24T17:16:38.846901Z","shell.execute_reply.started":"2025-06-24T17:16:38.828722Z","shell.execute_reply":"2025-06-24T17:16:38.845853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, tokenizer = ankh.load_large_model()\nmodel.eval()\nmodel.to(device=device)\nprint(f\"Number of parameters:\", get_num_params(model))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:16:38.849616Z","iopub.execute_input":"2025-06-24T17:16:38.849918Z","iopub.status.idle":"2025-06-24T17:17:20.500855Z","shell.execute_reply.started":"2025-06-24T17:16:38.849895Z","shell.execute_reply":"2025-06-24T17:17:20.499610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_sequence_id(index_name):\n    \"\"\"\n    Extract sequence ID from index like 'sequence_0_window_1'\n    \n    Parameters:\n    -----------\n    index_name : str\n        Index name in format 'sequence_[id]_window_[window_id]'\n    \n    Returns:\n    --------\n    str\n        The sequence ID (e.g., '0' from 'sequence_0_window_1')\n    \"\"\"\n    # Use regex to extract sequence ID\n    match = re.search(r'sequence_(\\d+)_window_\\d+', index_name)\n    if match:\n        return match.group(1)\n    else:\n        # Alternative pattern matching if format is different\n        parts = index_name.split('_')\n        if len(parts) >= 4 and parts[0] == 'sequence':\n            return parts[1]\n    return None\n\ndef sequence_based_split(df, val_size=0.2, random_state=42):\n    \"\"\"\n    Split dataframe based on sequence IDs to prevent data leakage.\n    All windows from the same sequence will be in the same split.\n    \n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        DataFrame with index containing sequence and window information\n    test_size : float\n        Proportion of sequences for test set\n    val_size : float\n        Proportion of remaining sequences for validation set\n    random_state : int\n        Random seed for reproducibility\n    \n    Returns:\n    --------\n    tuple\n        (train_df, val_df, test_df, split_info)\n    \"\"\"\n    # Extract sequence IDs from index\n    sequence_ids = []\n    for idx in df['id']:\n        seq_id = extract_sequence_id(str.lower(idx))\n        if seq_id is not None:\n            sequence_ids.append(seq_id)\n        else:\n            raise ValueError(f\"Could not extract sequence ID from index: {idx}\")\n    \n    # Add sequence IDs to dataframe (temporary)\n    df_temp = df.copy()\n    df_temp['sequence_id'] = sequence_ids\n    \n    # Get unique sequence IDs\n    unique_sequences = list(set(sequence_ids))\n    print(f\"Total unique sequences: {len(unique_sequences)}\")\n    \n    # Count windows per sequence\n    sequence_counts = defaultdict(int)\n    for seq_id in sequence_ids:\n        sequence_counts[seq_id] += 1\n    \n    print(f\"Windows per sequence: min={min(sequence_counts.values())}, \"\n          f\"max={max(sequence_counts.values())}, \"\n          f\"mean={np.mean(list(sequence_counts.values())):.1f}\")\n    \n    # First split: separate test sequences\n    train_sequences, val_sequences = train_test_split(\n        unique_sequences, \n        test_size=val_size, \n        random_state=random_state\n    )\n    \n    # Create splits based on sequence membership\n    train_df = df_temp[df_temp['sequence_id'].isin(train_sequences)].drop('sequence_id', axis=1)\n    val_df = df_temp[df_temp['sequence_id'].isin(val_sequences)].drop('sequence_id', axis=1)\n    \n    # Create split information\n    split_info = {\n        'train_sequences': sorted(train_sequences),\n        'val_sequences': sorted(val_sequences),\n        'train_windows': len(train_df),\n        'val_windows': len(val_df),\n        'total_windows': len(df)\n    }\n    \n    print(f\"Split results:\")\n    print(f\"  Train: {len(train_sequences)} sequences, {len(train_df)} windows\")\n    print(f\"  Val:   {len(val_sequences)} sequences, {len(val_df)} windows\") \n    \n    return train_df, val_df, split_info\n\ndef verify_no_leakage(train_df, val_df):\n    \"\"\"\n    Verify that there's no sequence leakage between splits.\n    \n    Parameters:\n    -----------\n    train_df, val_df, test_df : pandas.DataFrame\n        The split dataframes\n    \n    Returns:\n    --------\n    bool\n        True if no leakage detected, False otherwise\n    \"\"\"\n    # Extract sequence IDs from each split\n    train_seqs = set([extract_sequence_id(str(idx)) for idx in train_df.index])\n    val_seqs = set([extract_sequence_id(str(idx)) for idx in val_df.index])\n    \n    # Check for overlaps\n    train_val_overlap = list(train_seqs.intersection(val_seqs))\n    \n    if len(train_val_overlap) > 1 or train_val_overlap[0] is not None:\n        print(\"❌ DATA LEAKAGE DETECTED!\")\n        if train_val_overlap:\n            print(f\"  Train-Val overlap: {train_val_overlap}\")\n        return False\n    else:\n        print(\"✅ No data leakage detected. All sequences are properly separated.\")\n        return True\n\n# Example usage and demonstration\ndef split_dataset_process(df):\n    \"\"\"Demonstrate the sequence-based splitting functionality\"\"\"\n    \n    # Test regular sequence-based split\n    print(\"=== Regular Sequence-Based Split Process ===\")\n    train_df, val_df, split_info = sequence_based_split(\n        df, val_size=0.25, random_state=42\n    )\n    \n    print()\n    verify_no_leakage(train_df, val_df)\n    print()\n\n    \n    return train_df, val_df, split_info\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:17:20.502074Z","iopub.execute_input":"2025-06-24T17:17:20.502472Z","iopub.status.idle":"2025-06-24T17:17:20.525635Z","shell.execute_reply.started":"2025-06-24T17:17:20.502440Z","shell.execute_reply":"2025-06-24T17:17:20.524404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_pad = pd.read_csv('/kaggle/input/preprocessed-v4/PS4_108cut_Ankh.csv')\n# # df_pad = pd.read_csv('/kaggle/input/preprocessed-v4/PS4_108cut.csv')\n# print(\"=== PS4 Dataset ===\")\n# print(df_pad.info())\n# df_pad['len_seq_aa'] = df_pad['aa_sequence'].apply(lambda x : len(str(x)))\n# df_pad['len_ssp8'] = df_pad['ssp_sequence'].apply(lambda x : len(str(x)))\n# print(df_pad.describe())\n\n# train_df, val_df, split_info = split_dataset_process(df_pad)\n# print(\"=== Training PS4 Dataset ===\")\n# print(train_df.info())\n# print(\"=== Validation PS4 Dataset ===\")\n# print(val_df.info())\n\n# df_cb433 = pd.read_csv('/kaggle/input/preprocessed-v4/CB433_108cut_Ankh.csv')\n# # df_cb433 = pd.read_csv('/kaggle/input/preprocessed-v4/CB433_108cut.csv')\n# print(\"=== Test CB433 Dataset ===\")\n# print(df_cb433.info())\n# df_cb433['len_seq_aa'] = df_cb433['aa_sequence'].apply(lambda x : len(str(x)))\n# df_cb433['len_ssp8'] = df_cb433['ssp_sequence'].apply(lambda x : len(str(x)))\n# print(df_cb433.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:17:20.526796Z","iopub.execute_input":"2025-06-24T17:17:20.527168Z","iopub.status.idle":"2025-06-24T17:17:20.560410Z","shell.execute_reply.started":"2025-06-24T17:17:20.527134Z","shell.execute_reply":"2025-06-24T17:17:20.559367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def wrong_eos(df):\n    # --- Step 1: Fix sequences ending with '<', '/', 's' ---\n    df = df.copy()  # Avoid SettingWithCopyWarning\n    mask = df['aa_sequence'].str[-1].isin(['<', '/', 's'])\n\n    # Apply replacement based on last character\n    ends_with_s = mask & (df['aa_sequence'].str[-1] == 's')\n    ends_with_slash = mask & (df['aa_sequence'].str[-1] == '/')\n    ends_with_lt = mask & (df['aa_sequence'].str[-1] == '<')\n\n    # Use .loc to safely assign new values\n    df.loc[ends_with_s, 'aa_sequence'] = df.loc[ends_with_s, 'aa_sequence'].str[:-3] + '<eos>'\n    df.loc[ends_with_slash, 'aa_sequence'] = df.loc[ends_with_slash, 'aa_sequence'].str[:-2] + '<eos>'\n    df.loc[ends_with_lt, 'aa_sequence'] = df.loc[ends_with_lt, 'aa_sequence'].str[:-1] + '<eos>'\n\n    # --- Step 2: Drop rows where ssp_sequence is all 'X' ---\n    mask_ssp_all_x = df['ssp_sequence'].str.fullmatch(r'^X+$')\n    df = df[~mask_ssp_all_x].reset_index(drop=True)\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:17:20.561801Z","iopub.execute_input":"2025-06-24T17:17:20.562370Z","iopub.status.idle":"2025-06-24T17:17:20.582792Z","shell.execute_reply.started":"2025-06-24T17:17:20.562324Z","shell.execute_reply":"2025-06-24T17:17:20.581553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df =  pd.read_csv('/kaggle/input/preprocess-f/Train_PS4_Ankh.csv')\ntrain_df['len_seq_aa'] = train_df['aa_sequence'].apply(lambda x : len(str(x)))\ntrain_df['len_ssp8'] = train_df['ssp_sequence'].apply(lambda x : len(str(x)))\nprint(\"=== Training PS4 Dataset ===\")\nprint(train_df.info())\nval_df =  pd.read_csv('/kaggle/input/preprocess-f/Val_PS4_Ankh.csv')\nval_df['len_seq_aa'] = val_df['aa_sequence'].apply(lambda x : len(str(x)))\nval_df['len_ssp8'] = val_df['ssp_sequence'].apply(lambda x : len(str(x)))\nprint(\"=== Validation PS4 Dataset ===\")\nprint(val_df.info())\n\ndf_cb433 = pd.read_csv('/kaggle/input/preprocess-f/CB433_Ankh.csv')\n# df_cb433 = pd.read_csv('/kaggle/input/preprocessed-v4/CB433_108cut.csv')\nprint(\"=== Test CB433 Dataset ===\")\nprint(df_cb433.info())\ndf_cb433['len_seq_aa'] = df_cb433['aa_sequence'].apply(lambda x : len(str(x)))\ndf_cb433['len_ssp8'] = df_cb433['ssp_sequence'].apply(lambda x : len(str(x)))\nprint(df_cb433.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:17:20.583858Z","iopub.execute_input":"2025-06-24T17:17:20.584164Z","iopub.status.idle":"2025-06-24T17:17:21.274999Z","shell.execute_reply.started":"2025-06-24T17:17:20.584141Z","shell.execute_reply":"2025-06-24T17:17:21.274107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# replace J as unknown\ntrain_df['aa_sequence'] = train_df['aa_sequence'].str.replace('J', 'X')\nval_df['aa_sequence'] = val_df['aa_sequence'].str.replace('J', 'X')\ndf_cb433['aa_sequence']= df_cb433['aa_sequence'].str.replace('J', 'X')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:17:21.275868Z","iopub.execute_input":"2025-06-24T17:17:21.276173Z","iopub.status.idle":"2025-06-24T17:17:21.299012Z","shell.execute_reply.started":"2025-06-24T17:17:21.276153Z","shell.execute_reply":"2025-06-24T17:17:21.297937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# val_df = pd.read_csv('/kaggle/input/preprocessed-v4/Val_PS4.csv')\ndfs = [\n# train_df,\n       val_df,\ndf_cb433]\n# dfs = [val_df, df_cb433]\n\n# train_df.to_csv('Train_PS4.csv')\n# val_df.to_csv('Val_PS4.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:17:21.300163Z","iopub.execute_input":"2025-06-24T17:17:21.300567Z","iopub.status.idle":"2025-06-24T17:17:21.322092Z","shell.execute_reply.started":"2025-06-24T17:17:21.300514Z","shell.execute_reply":"2025-06-24T17:17:21.321318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Processing Dataset","metadata":{}},{"cell_type":"code","source":"def tokenize_sequence(sequence: str):\n        # Define the special tokens\n        # special_tokens = {\"<cls>\", \"<eos>\", \"<pad>\"}\n        special_tokens = [\"</s>\", \"<pad>\"]\n        # special_tokens = {\"[CLS]\", \"[PAD]\"}\n        result = []\n        i = 0\n        n = len(sequence)\n        \n        while i < n:\n            # Check if the current position starts a special token\n            if sequence[i:i+4] in special_tokens[0]:  # Handle </s>\n                result.append(sequence[i:i+4])\n                i += 4  # Skip the length of the special token\n            elif sequence[i:i+5] in special_tokens[1]:  # Handle <pad>\n                result.append(sequence[i:i+5])\n                i += 5  # Skip the length of the special token\n            else:\n                # Add individual characters\n                result.append(sequence[i])\n                i += 1\n    \n        return result\n    \ndef preprocessed(df_col):\n    processed_sequences = []\n    for seq in df_col:\n        # prep_seq = list(seq)\n        prep_seq = tokenize_sequence(seq)\n        processed_sequences.append(prep_seq)\n    return processed_sequences\n    \ndef preprocess_dataset(sequences, labels, \n                       # disorder, \n                       max_length=None, st = True):\n    \n    if st:\n        sequences = [\"\".join(str(seq).split()) for seq in sequences]\n    else:\n        sequences = preprocessed(sequences)\n    \n    if max_length is None:\n        max_length = len(max(sequences, key=lambda x: len(x)))\n\n    seqs = [list(seq)[:max_length] for seq in sequences]\n    \n    labels = [\"\".join(str(label).split()) for label in labels]\n    labels = [list(label)[:max_length] for label in labels]\n    \n    # disorder = [\" \".join(disorder.split()) for disorder in disorder]\n    # disorder = [disorder.split()[:max_length] for disorder in disorder]\n    \n    assert len(seqs) == len(labels)\n    return seqs, labels\n\n\ndef embed_dataset(model, sequences, \n                  # tokenizer, device, \n                  shift_left=0, shift_right=-1, max_length=108, st_status=True):\n    \"\"\"\n    Embed sequences using a pre-trained model and adjust the attention mask to only include \n    actual content (set to 0 after the last non-padding token).\n    \n    Args:\n        model: Pre-trained model for embedding the sequences\n        sequences: List of sequences to embed\n        tokenizer: Tokenizer to convert sequences to token IDs\n        device: Device to run the model on (e.g., 'cuda' or 'cpu')\n        shift_left: Number of tokens to exclude from the beginning\n        shift_right: Number of tokens to exclude from the end (negative indexing)\n        max_length: Maximum sequence length for padding/truncation\n        st_status: Whether to add special tokens\n        \n    Returns:\n        Tuple of (inputs_id, inputs_embedding, adjusted_attention_masks)\n    \"\"\"\n    inputs_embedding = []\n    inputs_id = []\n    adjusted_attention_masks = []\n    \n    with torch.no_grad():\n        for sample in tqdm(sequences):\n            # Tokenize the input\n            ids = tokenizer.batch_encode_plus(\n                [sample], \n                add_special_tokens=st_status, \n                max_length=max_length,\n                padding='max_length', \n                is_split_into_words=True, \n                truncation=True, \n                return_tensors=\"pt\"\n            )\n            \n            # Create a new attention mask based on the actual content\n            input_ids = ids['input_ids'][0].cpu().numpy()\n            \n            # Find the padding token ID (usually 0 or 1 depending on the tokenizer)\n            padding_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n            \n            # Find the position of the last non-padding token\n            non_padding_positions = np.where(input_ids != padding_token_id)[0]\n            \n            if len(non_padding_positions) > 0:\n                # If there are non-padding tokens, find the last one\n                last_token_pos = non_padding_positions[-1]\n                \n                # Create a new attention mask: 1s up to and including the last token, 0s after\n                adjusted_mask = np.zeros(max_length, dtype=np.int64)\n                adjusted_mask[:last_token_pos + 1] = 1\n            else:\n                # If all tokens are padding tokens (unlikely but possible)\n                adjusted_mask = np.zeros(max_length, dtype=np.int64)\n            \n            # Convert to tensor\n            ids['attention_mask'][0] = torch.tensor(adjusted_mask)\n            \n            # Generate embeddings\n            embedding = model(input_ids=ids['input_ids'].to(device))[0]\n            \n            # # Apply shifts if needed\n            # if shift_right == -1:\n            #     embedding_np = embedding[0].detach().cpu().numpy()[shift_left:]\n            # else:\n            #     embedding_np = embedding[0].detach().cpu().numpy()[shift_left:shift_right]\n            \n            inputs_embedding.append(embedding)\n            # inputs_id.append(ids)\n            # adjusted_attention_masks.append(adjusted_mask)\n    \n    return inputs_embedding\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:17:21.323340Z","iopub.execute_input":"2025-06-24T17:17:21.323668Z","iopub.status.idle":"2025-06-24T17:17:21.341420Z","shell.execute_reply.started":"2025-06-24T17:17:21.323647Z","shell.execute_reply":"2025-06-24T17:17:21.340239Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Processing Labels","metadata":{}},{"cell_type":"code","source":"# Consider each label as a tag for each token\n# training_sequences, training_labels = preprocess_dataset(df_pad['aa_sequence'], \n#                                                         df_pad['ssp_sequence'], \n#                                                          st = False\n#                                                         )\n# unique_tags = set(tag for doc in training_labels for tag in doc)\n# unique_tags.remove('X')\n# tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n# id2tag = {id: tag for tag, id in tag2id.items()}\n# del training_sequences, training_labels\n# print(tag2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:17:21.342570Z","iopub.execute_input":"2025-06-24T17:17:21.342824Z","iopub.status.idle":"2025-06-24T17:17:21.367724Z","shell.execute_reply.started":"2025-06-24T17:17:21.342806Z","shell.execute_reply":"2025-06-24T17:17:21.366677Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"{'T': 0, 'S': 1, 'G': 2, 'P': 3, 'L': 4, 'E': 5, 'B': 6, 'H': 7, 'I': 8}","metadata":{}},{"cell_type":"code","source":"\ndef encode_tags(labels, max_length=108, padding_value = -100):\n    \"\"\"\n    Encode tags to IDs and pad to a fixed length.\n    \n    Args:\n        labels: List of lists, where each inner list contains tags for a document\n        max_length: Maximum length for padding (default: 108)\n    \n    Returns:\n        List of lists with tag IDs and padding\n    \"\"\"\n    encoded_labels = []\n    \n    for doc in labels:\n        # Convert tags to IDs, skip unknown tags\n        doc_ids = [tag2id.get(tag, padding_value) for tag in doc]  # Use -100 for unknown tags\n        \n        # Truncate if longer than max_length\n        if len(doc_ids) > max_length:\n            doc_ids = doc_ids[:max_length]\n        # Add padding if shorter than max_length\n        else: \n            padding = [padding_value] * (max_length - len(doc_ids))\n            doc_ids = doc_ids + padding\n            \n        encoded_labels.append(doc_ids)\n    \n    return encoded_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:17:21.370789Z","iopub.execute_input":"2025-06-24T17:17:21.371095Z","iopub.status.idle":"2025-06-24T17:17:21.393856Z","shell.execute_reply.started":"2025-06-24T17:17:21.371073Z","shell.execute_reply":"2025-06-24T17:17:21.392771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef tensor_to_numpy(tensor):\n    \"\"\"Safely convert tensor to numpy array\"\"\"\n    # Check if it's a PyTorch tensor\n    if torch.is_tensor(tensor):\n        return tensor.detach().cpu().numpy()\n    elif isinstance(tensor, np.ndarray):\n        return tensor\n    elif isinstance(tensor, list):\n        # Handle list of tensors\n        converted_list = []\n        for item in tensor:\n            if torch.is_tensor(item):\n                converted_list.append(item.detach().cpu().numpy())\n            else:\n                converted_list.append(item)\n        return np.array(converted_list)\n    else:\n        # For other types, convert to numpy array safely\n        try:\n            return np.array(tensor)\n        except Exception as e:\n            print(f\"Error converting {type(tensor)} to numpy: {e}\")\n            raise\n\ndef memory_efficient_processing(model, df, tag2id, label, chunk_size=1000):\n    total_chunks = (len(df) - 1) // chunk_size + 1\n    results = []\n    \n    with tqdm(total=total_chunks, desc=\"Processing chunks\") as pbar:\n        input_column_name = 'aa_sequence'\n        labels_column_name = 'ssp_sequence'\n        padding_value = -100\n        # Create output directory\n        output_dir = './embedding_Ankh_result/'\n        os.makedirs(output_dir, exist_ok=True)\n\n        for i in range(0, len(df), chunk_size):\n            # Get chunk\n            chunk_num = i // chunk_size\n            chunk = df.iloc[i:i + chunk_size].copy()\n            print(f\"Processing chunk {chunk_num + 1}/{(len(df)-1)//chunk_size + 1}\")\n\n            #preprocessing dataset\n            training_sequences = chunk[input_column_name]\n            training_labels = chunk[labels_column_name]\n\n            training_sequences, training_labels = preprocess_dataset(training_sequences, \n                                                        training_labels, \n                                                         st = False\n                                                        )\n             # Clear memory\n            del chunk\n            # Process embeddings\n            training_embeddings = embed_dataset(model, \n                                                training_sequences,\n                                                st_status = False)\n             # Clear memory\n            del training_sequences\n            # Save to NPZ\n            npz_path = os.path.join(output_dir, f'{label}_sequence_chunk_{chunk_num:04d}.npz')\n            \n            # Save embeddings and any metadata\n            np.savez_compressed(\n                npz_path,\n                embeddings=tensor_to_numpy(training_embeddings),\n                # indices=chunk.index.values,  # Original indices\n                chunk_start=i,\n                chunk_end=min(i + chunk_size, len(df))\n            )\n            del training_embeddings\n            \n            #process label encoding\n            train_labels_encodings = torch.tensor(encode_tags(training_labels, padding_value = padding_value))\n            train_tensor_enc = F.one_hot(train_labels_encodings.clamp(min=0), num_classes=9)\n            # Replace one-hot vectors for padding values with all zeros\n            # This creates a mask where True indicates padding positions\n            padding_mask = (train_labels_encodings == padding_value).unsqueeze(-1)\n            # Zero out the one-hot vectors at padding positions\n            train_tensor_enc = train_tensor_enc.masked_fill(padding_mask, 0)\n            print(f\"Final shape: {train_tensor_enc.shape}\") \n\n            del training_labels\n\n            # Save to NPZ\n            npz_path = os.path.join(output_dir, f'{label}_labels_chunk_{chunk_num:04d}.npz')\n            \n            # Save embeddings and any metadata\n            np.savez_compressed(\n                npz_path,\n                embeddings=tensor_to_numpy(train_tensor_enc),\n                # indices=chunk.index.values,  # Original indices\n                chunk_start=i,\n                chunk_end=min(i + chunk_size, len(df))\n            )\n            # Clear memory\n            del train_labels_encodings, train_tensor_enc, padding_mask\n            \n            # Update progress\n            pbar.update(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:17:21.394837Z","iopub.execute_input":"2025-06-24T17:17:21.395119Z","iopub.status.idle":"2025-06-24T17:17:21.410808Z","shell.execute_reply.started":"2025-06-24T17:17:21.395099Z","shell.execute_reply":"2025-06-24T17:17:21.409552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tag2id = {'S': 0, 'H': 1, 'B': 2, 'I': 3, 'L': 4, 'P': 5, 'G': 6, 'E': 7, 'T': 8}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:17:21.411953Z","iopub.execute_input":"2025-06-24T17:17:21.412285Z","iopub.status.idle":"2025-06-24T17:17:21.437626Z","shell.execute_reply.started":"2025-06-24T17:17:21.412256Z","shell.execute_reply":"2025-06-24T17:17:21.436305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_dfs = ['train', 'val', 'test']\n# label_dfs = ['val']\nfor i in range(len(dfs)):\n    memory_efficient_processing(model, dfs[i], tag2id, label = label_dfs[i], chunk_size=5000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:17:21.438815Z","iopub.execute_input":"2025-06-24T17:17:21.439114Z","iopub.status.idle":"2025-06-24T17:18:12.294204Z","shell.execute_reply.started":"2025-06-24T17:17:21.439091Z","shell.execute_reply":"2025-06-24T17:18:12.292486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def merge_npz_files(input_pattern, output_file, merge_arrays=True):\n    \"\"\"\n    Merge multiple NPZ files into one\n    \n    Args:\n        input_pattern: Pattern to match files (e.g., \"train_*.npz\" or list of filenames)\n        output_file: Output NPZ filename\n        merge_arrays: If True, concatenate arrays with same keys. If False, keep separate with indices\n    \"\"\"\n    # Get list of files\n    if isinstance(input_pattern, str):\n        npz_files = sorted(glob.glob(input_pattern))\n    else:\n        npz_files = input_pattern\n    \n    print(f\"Found {len(npz_files)} NPZ files to merge:\")\n    for f in npz_files:\n        print(f\"  - {f}\")\n    \n    if not npz_files:\n        print(\"No NPZ files found!\")\n        return\n    \n    # Load first file to get structure\n    with np.load(npz_files[0]) as first_file:\n        array_names = list(first_file.keys())\n        print(f\"Array keys found: {array_names}\")\n    \n    if merge_arrays:\n        # Method 1: Concatenate arrays with same keys\n        merged_data = {}\n        \n        for array_name in array_names:\n            arrays_to_merge = []\n            \n            for npz_file in npz_files:\n                with np.load(npz_file) as data:\n                    if array_name in data:\n                        arrays_to_merge.append(data[array_name])\n                        print(f\"Loading {array_name} from {npz_file}: shape {data[array_name].shape}\")\n            \n            if arrays_to_merge:\n                # Concatenate along first axis (assuming batch dimension)\n                merged_array = np.concatenate(arrays_to_merge, axis=0)\n                merged_data[array_name] = merged_array\n                print(f\"Merged {array_name}: final shape {merged_array.shape}\")\n        \n        # Save merged data\n        np.savez_compressed(output_file, **merged_data)\n        print(f\"Saved merged data to {output_file}\")\n        \n    else:\n        # Method 2: Keep arrays separate with file indices\n        merged_data = {}\n        \n        for i, npz_file in enumerate(npz_files):\n            with np.load(npz_file) as data:\n                for array_name in data.keys():\n                    key = f\"{array_name}_file_{i}\"\n                    merged_data[key] = data[array_name]\n                    print(f\"Added {key}: shape {data[array_name].shape}\")\n        \n        # Save with indexed keys\n        np.savez_compressed(output_file, **merged_data)\n        print(f\"Saved indexed data to {output_file}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:18:12.295084Z","iopub.status.idle":"2025-06-24T17:18:12.295419Z","shell.execute_reply.started":"2025-06-24T17:18:12.295275Z","shell.execute_reply":"2025-06-24T17:18:12.295289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# output_dir = '/kaggle/working/embedding_Ankh_result/'\n# \"\"\"Merge specific files by name\"\"\"\n# files_to_merge = [\n#         # output_dir + \"train_labels_chunk_0000.npz\",\n#         # output_dir + \"train_labels_chunk_0001.npz\", \n#         output_dir + \"train_sequence_chunk_0000.npz\",\n#         output_dir + \"train_sequence_chunk_0001.npz\"\n#     ]\n\n# merge_npz_files(files_to_merge, \"merged_specific.npz\", merge_arrays=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:18:12.296768Z","iopub.status.idle":"2025-06-24T17:18:12.297192Z","shell.execute_reply.started":"2025-06-24T17:18:12.296988Z","shell.execute_reply":"2025-06-24T17:18:12.297007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:18:12.299135Z","iopub.status.idle":"2025-06-24T17:18:12.299477Z","shell.execute_reply.started":"2025-06-24T17:18:12.299338Z","shell.execute_reply":"2025-06-24T17:18:12.299351Z"}},"outputs":[],"execution_count":null}]}