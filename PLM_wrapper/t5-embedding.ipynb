{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12271810,"sourceType":"datasetVersion","datasetId":7733372}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:48:05.355199Z","iopub.execute_input":"2025-06-30T04:48:05.355451Z","iopub.status.idle":"2025-06-30T04:48:06.276299Z","shell.execute_reply.started":"2025-06-30T04:48:05.355425Z","shell.execute_reply":"2025-06-30T04:48:06.275539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch transformers sentencepiece h5py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:48:06.278248Z","iopub.execute_input":"2025-06-30T04:48:06.278536Z","iopub.status.idle":"2025-06-30T04:49:16.268616Z","shell.execute_reply.started":"2025-06-30T04:48:06.278520Z","shell.execute_reply":"2025-06-30T04:49:16.267908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget -nc -P protT5/sec_struct_checkpoint http://data.bioembeddings.com/public/embeddings/feature_models/t5/secstruct_checkpoint.pt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:49:16.269437Z","iopub.execute_input":"2025-06-30T04:49:16.269638Z","iopub.status.idle":"2025-06-30T04:49:17.100297Z","shell.execute_reply.started":"2025-06-30T04:49:16.269616Z","shell.execute_reply":"2025-06-30T04:49:17.099570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# whether to retrieve embeddings for each residue in a protein\n# --> Lx1024 matrix per protein with L being the protein's length\n# as a rule of thumb: 1k proteins require around 1GB RAM/disk\nper_residue = True\nper_residue_path = \"./protT5/output/per_residue_embeddings.h5\" # where to store the embeddings\n\n# whether to retrieve per-protein embeddings\n# --> only one 1024-d vector per protein, irrespective of its length\nper_protein = True\nper_protein_path = \"./protT5/output/per_protein_embeddings.h5\" # where to store the embeddings\n\n# make sure that either per-residue or per-protein embeddings are stored\nassert per_protein is True or per_residue is True or sec_struct is True, print(\n    \"Minimally, you need to active per_residue, per_protein or sec_struct. (or any combination)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:49:17.101279Z","iopub.execute_input":"2025-06-30T04:49:17.101482Z","iopub.status.idle":"2025-06-30T04:49:17.106407Z","shell.execute_reply.started":"2025-06-30T04:49:17.101460Z","shell.execute_reply":"2025-06-30T04:49:17.105679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_sec_struct_model():\n  checkpoint_dir=\"./protT5/sec_struct_checkpoint/secstruct_checkpoint.pt\"\n  state = torch.load( checkpoint_dir )\n  model = ConvNet()\n  model.load_state_dict(state['state_dict'])\n  model = model.eval()\n  model = model.to(device)\n  print('Loaded sec. struct. model from epoch: {:.1f}'.format(state['epoch']))\n\n  return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:49:17.107245Z","iopub.execute_input":"2025-06-30T04:49:17.107407Z","iopub.status.idle":"2025-06-30T04:49:17.122996Z","shell.execute_reply.started":"2025-06-30T04:49:17.107394Z","shell.execute_reply":"2025-06-30T04:49:17.122340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Generate embeddings via batch-processing\n# per_residue indicates that embeddings for each residue in a protein should be returned.\n# per_protein indicates that embeddings for a whole protein should be returned (average-pooling)\n# max_residues gives the upper limit of residues within one batch\n# max_seq_len gives the upper sequences length for applying batch-processing\n# max_batch gives the upper number of sequences per batch\ndef get_embeddings( model, tokenizer, seqs, per_residue, per_protein, sec_struct,\n                   max_residues=4000, max_seq_len=1000, max_batch=100 ):\n\n    if sec_struct:\n      sec_struct_model = load_sec_struct_model()\n\n    results = {\"residue_embs\" : dict(),\n               \"protein_embs\" : dict(),\n               \"sec_structs\" : dict()\n               }\n\n    # sort sequences according to length (reduces unnecessary padding --> speeds up embedding)\n    seq_dict   = sorted( seqs.items(), key=lambda kv: len( seqs[kv[0]] ), reverse=True )\n    start = time.time()\n    batch = list()\n    for seq_idx, (pdb_id, seq) in enumerate(seq_dict,1):\n        seq = seq\n        seq_len = len(seq)\n        seq = ' '.join(list(seq))\n        batch.append((pdb_id,seq,seq_len))\n\n        # count residues in current batch and add the last sequence length to\n        # avoid that batches with (n_res_batch > max_residues) get processed\n        n_res_batch = sum([ s_len for  _, _, s_len in batch ]) + seq_len\n        if len(batch) >= max_batch or n_res_batch>=max_residues or seq_idx==len(seq_dict) or seq_len>max_seq_len:\n            pdb_ids, seqs, seq_lens = zip(*batch)\n            batch = list()\n\n            # add_special_tokens adds extra token at the end of each sequence\n            token_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n            input_ids      = torch.tensor(token_encoding['input_ids']).to(device)\n            attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n\n            try:\n                with torch.no_grad():\n                    # returns: ( batch-size x max_seq_len_in_minibatch x embedding_dim )\n                    embedding_repr = model(input_ids, attention_mask=attention_mask)\n            except RuntimeError:\n                print(\"RuntimeError during embedding for {} (L={})\".format(pdb_id, seq_len))\n                continue\n\n            if sec_struct: # in case you want to predict secondary structure from embeddings\n              d3_Yhat, d8_Yhat, diso_Yhat = sec_struct_model(embedding_repr.last_hidden_state)\n\n\n            for batch_idx, identifier in enumerate(pdb_ids): # for each protein in the current mini-batch\n                s_len = seq_lens[batch_idx]\n                # slice off padding --> batch-size x seq_len x embedding_dim\n                emb = embedding_repr.last_hidden_state[batch_idx,:s_len]\n                if sec_struct: # get classification results\n                    results[\"sec_structs\"][identifier] = torch.max( d3_Yhat[batch_idx,:s_len], dim=1 )[1].detach().cpu().numpy().squeeze()\n                if per_residue: # store per-residue embeddings (Lx1024)\n                    results[\"residue_embs\"][ identifier ] = emb.detach().cpu().numpy().squeeze()\n                if per_protein: # apply average-pooling to derive per-protein embeddings (1024-d)\n                    protein_emb = emb.mean(dim=0)\n                    results[\"protein_embs\"][identifier] = protein_emb.detach().cpu().numpy().squeeze()\n\n\n    passed_time=time.time()-start\n    avg_time = passed_time/len(results[\"residue_embs\"]) if per_residue else passed_time/len(results[\"protein_embs\"])\n    print('\\n############# EMBEDDING STATS #############')\n    print('Total number of per-residue embeddings: {}'.format(len(results[\"residue_embs\"])))\n    print('Total number of per-protein embeddings: {}'.format(len(results[\"protein_embs\"])))\n    print(\"Time for generating embeddings: {:.1f}[m] ({:.3f}[s/protein])\".format(\n        passed_time/60, avg_time ))\n    print('\\n############# END #############')\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:49:17.123864Z","iopub.execute_input":"2025-06-30T04:49:17.124061Z","iopub.status.idle":"2025-06-30T04:49:17.146062Z","shell.execute_reply.started":"2025-06-30T04:49:17.124046Z","shell.execute_reply":"2025-06-30T04:49:17.145448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import T5EncoderModel, T5Tokenizer\n\nimport h5py\nimport time\n\n\nimport os\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom collections import defaultdict\nimport random\n\nseed = 7\n\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import functional as F\n\nfrom transformers import Trainer, TrainingArguments, EvalPrediction\nfrom datasets import load_dataset\n\n# from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom scipy import stats\nfrom functools import partial\nimport pandas as pd\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:49:17.148191Z","iopub.execute_input":"2025-06-30T04:49:17.148394Z","iopub.status.idle":"2025-06-30T04:49:43.828800Z","shell.execute_reply.started":"2025-06-30T04:49:17.148378Z","shell.execute_reply":"2025-06-30T04:49:43.828026Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Model and Data","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Available device:', device)\ndef check_len(some_list):\n    for i in some_list:\n        print(len(i))\n\n\ndef get_num_params(model):\n    return sum(p.numel() for p in model.parameters())\n\ndef get_T5_model():\n    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n    model = model.to(device) # move model to GPU\n    model = model.eval() # set model to evaluation model\n    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n\n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:49:43.829563Z","iopub.execute_input":"2025-06-30T04:49:43.830155Z","iopub.status.idle":"2025-06-30T04:49:43.836084Z","shell.execute_reply.started":"2025-06-30T04:49:43.830133Z","shell.execute_reply":"2025-06-30T04:49:43.835244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, tokenizer = get_T5_model()\nmodel.eval()\nmodel.to(device=device)\nprint(f\"Number of parameters:\", get_num_params(model))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:49:43.837353Z","iopub.execute_input":"2025-06-30T04:49:43.837680Z","iopub.status.idle":"2025-06-30T04:50:29.439463Z","shell.execute_reply.started":"2025-06-30T04:49:43.837652Z","shell.execute_reply":"2025-06-30T04:50:29.438748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_sequence_id(index_name):\n    \"\"\"\n    Extract sequence ID from index like 'sequence_0_window_1'\n    \n    Parameters:\n    -----------\n    index_name : str\n        Index name in format 'sequence_[id]_window_[window_id]'\n    \n    Returns:\n    --------\n    str\n        The sequence ID (e.g., '0' from 'sequence_0_window_1')\n    \"\"\"\n    # Use regex to extract sequence ID\n    match = re.search(r'sequence_(\\d+)_window_\\d+', index_name)\n    if match:\n        return match.group(1)\n    else:\n        # Alternative pattern matching if format is different\n        parts = index_name.split('_')\n        if len(parts) >= 4 and parts[0] == 'sequence':\n            return parts[1]\n    return None\n\ndef sequence_based_split(df, val_size=0.2, random_state=42):\n    \"\"\"\n    Split dataframe based on sequence IDs to prevent data leakage.\n    All windows from the same sequence will be in the same split.\n    \n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        DataFrame with index containing sequence and window information\n    test_size : float\n        Proportion of sequences for test set\n    val_size : float\n        Proportion of remaining sequences for validation set\n    random_state : int\n        Random seed for reproducibility\n    \n    Returns:\n    --------\n    tuple\n        (train_df, val_df, test_df, split_info)\n    \"\"\"\n    # Extract sequence IDs from index\n    sequence_ids = []\n    for idx in df['id']:\n        seq_id = extract_sequence_id(str.lower(idx))\n        if seq_id is not None:\n            sequence_ids.append(seq_id)\n        else:\n            raise ValueError(f\"Could not extract sequence ID from index: {idx}\")\n    \n    # Add sequence IDs to dataframe (temporary)\n    df_temp = df.copy()\n    df_temp['sequence_id'] = sequence_ids\n    \n    # Get unique sequence IDs\n    unique_sequences = list(set(sequence_ids))\n    print(f\"Total unique sequences: {len(unique_sequences)}\")\n    \n    # Count windows per sequence\n    sequence_counts = defaultdict(int)\n    for seq_id in sequence_ids:\n        sequence_counts[seq_id] += 1\n    \n    print(f\"Windows per sequence: min={min(sequence_counts.values())}, \"\n          f\"max={max(sequence_counts.values())}, \"\n          f\"mean={np.mean(list(sequence_counts.values())):.1f}\")\n    \n    # First split: separate test sequences\n    train_sequences, val_sequences = train_test_split(\n        unique_sequences, \n        test_size=val_size, \n        random_state=random_state\n    )\n    \n    # Create splits based on sequence membership\n    train_df = df_temp[df_temp['sequence_id'].isin(train_sequences)].drop('sequence_id', axis=1)\n    val_df = df_temp[df_temp['sequence_id'].isin(val_sequences)].drop('sequence_id', axis=1)\n    \n    # Create split information\n    split_info = {\n        'train_sequences': sorted(train_sequences),\n        'val_sequences': sorted(val_sequences),\n        'train_windows': len(train_df),\n        'val_windows': len(val_df),\n        'total_windows': len(df)\n    }\n    \n    print(f\"Split results:\")\n    print(f\"  Train: {len(train_sequences)} sequences, {len(train_df)} windows\")\n    print(f\"  Val:   {len(val_sequences)} sequences, {len(val_df)} windows\") \n    \n    return train_df, val_df, split_info\n\ndef verify_no_leakage(train_df, val_df):\n    \"\"\"\n    Verify that there's no sequence leakage between splits.\n    \n    Parameters:\n    -----------\n    train_df, val_df, test_df : pandas.DataFrame\n        The split dataframes\n    \n    Returns:\n    --------\n    bool\n        True if no leakage detected, False otherwise\n    \"\"\"\n    # Extract sequence IDs from each split\n    train_seqs = set([extract_sequence_id(str(idx)) for idx in train_df.index])\n    val_seqs = set([extract_sequence_id(str(idx)) for idx in val_df.index])\n    \n    # Check for overlaps\n    train_val_overlap = list(train_seqs.intersection(val_seqs))\n    \n    if len(train_val_overlap) > 1 or train_val_overlap[0] is not None:\n        print(\"❌ DATA LEAKAGE DETECTED!\")\n        if train_val_overlap:\n            print(f\"  Train-Val overlap: {train_val_overlap}\")\n        return False\n    else:\n        print(\"✅ No data leakage detected. All sequences are properly separated.\")\n        return True\n\n# Example usage and demonstration\ndef split_dataset_process(df):\n    \"\"\"Demonstrate the sequence-based splitting functionality\"\"\"\n    \n    # Test regular sequence-based split\n    print(\"=== Regular Sequence-Based Split Process ===\")\n    train_df, val_df, split_info = sequence_based_split(\n        df, val_size=0.25, random_state=42\n    )\n    \n    print()\n    verify_no_leakage(train_df, val_df)\n    print()\n\n    \n    return train_df, val_df, split_info\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:50:29.440317Z","iopub.execute_input":"2025-06-30T04:50:29.440515Z","iopub.status.idle":"2025-06-30T04:50:29.451896Z","shell.execute_reply.started":"2025-06-30T04:50:29.440491Z","shell.execute_reply":"2025-06-30T04:50:29.451277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df =  pd.read_csv('/kaggle/input/preprocess-f/Train_PS4_T5.csv')\ntrain_df['len_seq_aa'] = train_df['aa_sequence'].apply(lambda x : len(str(x)))\ntrain_df['len_ssp8'] = train_df['ssp_sequence'].apply(lambda x : len(str(x)))\nprint(\"=== Training PS4 Dataset ===\")\nprint(train_df.info())\nval_df =  pd.read_csv('/kaggle/input/preprocess-f/Val_PS4_T5.csv')\nval_df['len_seq_aa'] = val_df['aa_sequence'].apply(lambda x : len(str(x)))\nval_df['len_ssp8'] = val_df['ssp_sequence'].apply(lambda x : len(str(x)))\nprint(\"=== Validation PS4 Dataset ===\")\nprint(val_df.info())\n\ndf_cb433 = pd.read_csv('/kaggle/input/preprocess-f/CB433_T5.csv')\n# df_cb433 = pd.read_csv('/kaggle/input/preprocessed-v4/CB433_108cut.csv')\nprint(\"=== Test CB433 Dataset ===\")\nprint(df_cb433.info())\ndf_cb433['len_seq_aa'] = df_cb433['aa_sequence'].apply(lambda x : len(str(x)))\ndf_cb433['len_ssp8'] = df_cb433['ssp_sequence'].apply(lambda x : len(str(x)))\nprint(df_cb433.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:50:29.452681Z","iopub.execute_input":"2025-06-30T04:50:29.452877Z","iopub.status.idle":"2025-06-30T04:50:31.157269Z","shell.execute_reply.started":"2025-06-30T04:50:29.452863Z","shell.execute_reply":"2025-06-30T04:50:31.156599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# val_df = pd.read_csv('/kaggle/input/preprocessed-v4/Val_PS4.csv')\ndfs = [\ntrain_df,\n       val_df,\ndf_cb433]\n# dfs = [val_df, df_cb433]\n\n# train_df.to_csv('Train_PS4.csv')\n# val_df.to_csv('Val_PS4.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:50:31.157929Z","iopub.execute_input":"2025-06-30T04:50:31.158156Z","iopub.status.idle":"2025-06-30T04:50:31.161577Z","shell.execute_reply.started":"2025-06-30T04:50:31.158139Z","shell.execute_reply":"2025-06-30T04:50:31.160833Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Processing Dataset","metadata":{}},{"cell_type":"code","source":"def tokenize_sequence(sequence: str):\n        # Define the special tokens\n        # special_tokens = {\"<cls>\", \"<eos>\", \"<pad>\"}\n        special_tokens = [\"</s>\", \"<pad>\"]\n        # special_tokens = {\"[CLS]\", \"[PAD]\"}\n        result = []\n        i = 0\n        n = len(sequence)\n        limit = 108\n        while i < n:\n            # Check if the current position starts a special token\n            if sequence[i:i+4] in special_tokens[0]:  # Handle </s>\n                result.append(sequence[i:i+4])\n                i += 4  # Skip the length of the special token\n            elif sequence[i:i+5] in special_tokens[1]:  # Handle <pad>\n                result.append(sequence[i:i+5])\n                i += 5  # Skip the length of the special token\n            else:\n                # Add individual characters\n                result.append(sequence[i])\n                i += 1\n        if len(result) < limit:\n            num_pad = limit- len(result)\n            result += ['<pad>']*num_pad\n        if len(result)> limit:\n            result = result[:limit]\n    \n        return result\n    \ndef preprocessed(df_col):\n    processed_sequences = []\n    for seq in df_col:\n        # prep_seq = list(seq)\n        prep_seq = tokenize_sequence(seq)\n        processed_sequences.append(prep_seq)\n    return processed_sequences\n    \ndef preprocess_dataset(sequences, labels, \n                       # disorder, \n                       max_length=None, st = True):\n    \n    if st:\n        sequences = [\"\".join(str(seq).split()) for seq in sequences]\n    else:\n        sequences = preprocessed(sequences)\n    \n    if max_length is None:\n        max_length = len(max(sequences, key=lambda x: len(x)))\n\n    seqs = [list(seq)[:max_length] for seq in sequences]\n    \n    labels = [\"\".join(str(label).split()) for label in labels]\n    labels = [list(label)[:max_length] for label in labels]\n    \n    # disorder = [\" \".join(disorder.split()) for disorder in disorder]\n    # disorder = [disorder.split()[:max_length] for disorder in disorder]\n    \n    assert len(seqs) == len(labels)\n    return seqs, labels\n\n\ndef embed_dataset(model, sequences, \n                  # tokenizer, device, \n                  shift_left=0, shift_right=-1, max_length=108, st_status=True):\n    \"\"\"\n    Embed sequences using a pre-trained model and adjust the attention mask to only include \n    actual content (set to 0 after the last non-padding token).\n    \n    Args:\n        model: Pre-trained model for embedding the sequences\n        sequences: List of sequences to embed\n        tokenizer: Tokenizer to convert sequences to token IDs\n        device: Device to run the model on (e.g., 'cuda' or 'cpu')\n        shift_left: Number of tokens to exclude from the beginning\n        shift_right: Number of tokens to exclude from the end (negative indexing)\n        max_length: Maximum sequence length for padding/truncation\n        st_status: Whether to add special tokens\n        \n    Returns:\n        Tuple of (inputs_id, inputs_embedding, adjusted_attention_masks)\n    \"\"\"\n    inputs_embedding = []\n    inputs_id = []\n    adjusted_attention_masks = []\n    \n    with torch.no_grad():\n        for sample in tqdm(sequences):\n            # Tokenize the input\n            ids = tokenizer.batch_encode_plus(\n                sample, \n                add_special_tokens=True, \n                # max_length=max_length,\n                padding=True, \n                # is_split_into_words=True, \n                truncation=True, \n                return_tensors=\"pt\"\n            )\n            \n            # Create a new attention mask based on the actual content\n            input_ids = torch.tensor(ids['input_ids']).to(device)\n            \n            # Find the padding token ID (usually 0 or 1 depending on the tokenizer)\n            padding_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n            \n            # # Find the position of the last non-padding token\n            # non_padding_positions = np.where(input_ids != padding_token_id)[0]\n            \n            # if len(non_padding_positions) > 0:\n            #     # If there are non-padding tokens, find the last one\n            #     last_token_pos = non_padding_positions[-1]\n                \n            #     # Create a new attention mask: 1s up to and including the last token, 0s after\n            #     adjusted_mask = np.zeros(max_length, dtype=np.int64)\n            #     adjusted_mask[:last_token_pos + 1] = 1\n            # else:\n            #     # If all tokens are padding tokens (unlikely but possible)\n            #     adjusted_mask = np.zeros(max_length, dtype=np.int64)\n            \n            # Convert to tensor\n            attention_mask = torch.tensor(ids['attention_mask']).to(device)\n            \n            # Generate embeddings\n            embedding = model.encoder(input_ids, attention_mask=attention_mask)\n            \n            # # Apply shifts if needed\n            # if shift_right == -1:\n            #     embedding_np = embedding[0].detach().cpu().numpy()[shift_left:]\n            # else:\n            #     embedding_np = embedding[0].detach().cpu().numpy()[shift_left:shift_right]\n            \n            inputs_embedding.append(embedding.last_hidden_state.to(device))\n            # inputs_id.append(ids)\n            # adjusted_attention_masks.append(adjusted_mask)\n    \n    return inputs_embedding\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:04:33.484284Z","iopub.execute_input":"2025-06-30T06:04:33.485027Z","iopub.status.idle":"2025-06-30T06:04:33.496953Z","shell.execute_reply.started":"2025-06-30T06:04:33.485006Z","shell.execute_reply":"2025-06-30T06:04:33.496279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Processing Labels","metadata":{}},{"cell_type":"code","source":"# Consider each label as a tag for each token\n# training_sequences, training_labels = preprocess_dataset(df_pad['aa_sequence'], \n#                                                         df_pad['ssp_sequence'], \n#                                                          st = False\n#                                                         )\n# unique_tags = set(tag for doc in training_labels for tag in doc)\n# unique_tags.remove('X')\n# tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n# id2tag = {id: tag for tag, id in tag2id.items()}\n# del training_sequences, training_labels\n# print(tag2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:50:31.181035Z","iopub.execute_input":"2025-06-30T04:50:31.181531Z","iopub.status.idle":"2025-06-30T04:50:31.199037Z","shell.execute_reply.started":"2025-06-30T04:50:31.181515Z","shell.execute_reply":"2025-06-30T04:50:31.198328Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"{'T': 0, 'S': 1, 'G': 2, 'P': 3, 'L': 4, 'E': 5, 'B': 6, 'H': 7, 'I': 8}","metadata":{}},{"cell_type":"code","source":"\ndef encode_tags(labels, max_length=108, padding_value = -100):\n    \"\"\"\n    Encode tags to IDs and pad to a fixed length.\n    \n    Args:\n        labels: List of lists, where each inner list contains tags for a document\n        max_length: Maximum length for padding (default: 108)\n    \n    Returns:\n        List of lists with tag IDs and padding\n    \"\"\"\n    encoded_labels = []\n    \n    for doc in labels:\n        # Convert tags to IDs, skip unknown tags\n        doc_ids = [tag2id.get(tag, padding_value) for tag in doc]  # Use -100 for unknown tags\n        \n        # Truncate if longer than max_length\n        if len(doc_ids) > max_length:\n            doc_ids = doc_ids[:max_length]\n        # Add padding if shorter than max_length\n        else: \n            padding = [padding_value] * (max_length - len(doc_ids))\n            doc_ids = doc_ids + padding\n            \n        encoded_labels.append(doc_ids)\n    \n    return encoded_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:50:31.199744Z","iopub.execute_input":"2025-06-30T04:50:31.199982Z","iopub.status.idle":"2025-06-30T04:50:31.214772Z","shell.execute_reply.started":"2025-06-30T04:50:31.199959Z","shell.execute_reply":"2025-06-30T04:50:31.214107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef tensor_to_numpy(tensor):\n    \"\"\"Safely convert tensor to numpy array\"\"\"\n    # Check if it's a PyTorch tensor\n    if torch.is_tensor(tensor):\n        return tensor.detach().cpu().numpy()\n    elif isinstance(tensor, np.ndarray):\n        return tensor\n    elif isinstance(tensor, list):\n        # Handle list of tensors\n        converted_list = []\n        for item in tensor:\n            if torch.is_tensor(item):\n                converted_list.append(item.detach().cpu().numpy())\n            else:\n                converted_list.append(item)\n        return np.array(converted_list)\n    else:\n        # For other types, convert to numpy array safely\n        try:\n            return np.array(tensor)\n        except Exception as e:\n            print(f\"Error converting {type(tensor)} to numpy: {e}\")\n            raise\n# processing to numpy special because it got error terus menerus naon gw gatau\ndef convert_embeddings_to_numpy(training_embeddings):\n    \"\"\"Convert list of same-shaped tensors to numpy array\"\"\"\n    # Convert each tensor individually first\n    numpy_embeddings = []\n    for i, embedding in enumerate(training_embeddings):\n        if torch.is_tensor(embedding):\n            numpy_emb = embedding.detach().cpu().numpy()\n            # print(f\"Converted tensor {i}: shape {numpy_emb.shape}, dtype {numpy_emb.dtype}\")\n            numpy_embeddings.append(numpy_emb)\n        else:\n            print(f\"Item {i} is not a tensor: {type(embedding)}\")\n            numpy_embeddings.append(embedding)\n    \n    # Stack them into a single array\n    try:\n        stacked = np.stack(numpy_embeddings, axis=0)\n        # print(f\"Final stacked shape: {stacked.shape}\")\n        return stacked\n    except Exception as e:\n        # print(f\"Error stacking: {e}\")\n        return numpy_embeddings\n        \ndef memory_efficient_processing(model, df, tag2id, label, chunk_size=1000):\n    total_chunks = (len(df) - 1) // chunk_size + 1\n    results = []\n    \n    with tqdm(total=total_chunks, desc=\"Processing chunks\") as pbar:\n        input_column_name = 'aa_sequence'\n        labels_column_name = 'ssp_sequence'\n        padding_value = -100\n        # Create output directory\n        output_dir = './embedding_T5_result/'\n        os.makedirs(output_dir, exist_ok=True)\n\n        for i in range(0, len(df), chunk_size):\n            # Get chunk\n            chunk_num = i // chunk_size\n            chunk = df.iloc[i:i + chunk_size].copy()\n            print(f\"Processing chunk {chunk_num + 1}/{(len(df)-1)//chunk_size + 1}\")\n\n            #preprocessing dataset\n            training_sequences = chunk[input_column_name]\n            training_labels = chunk[labels_column_name]\n\n            training_sequences, training_labels = preprocess_dataset(training_sequences, \n                                                        training_labels, \n                                                         st = False\n                                                        )\n             # Clear memory\n            del chunk\n            # Process embeddings\n            training_embeddings = embed_dataset(model, \n                                                training_sequences,\n                                                st_status = True)\n             # Clear memory\n            del training_sequences\n            # Save to NPZ\n            npz_path = os.path.join(output_dir, f'{label}_sequence_chunk_{chunk_num:04d}.npz')\n            \n            \n            # Save embeddings and any metadata\n            np.savez_compressed(\n                npz_path,\n                embeddings=convert_embeddings_to_numpy(training_embeddings),\n                # indices=chunk.index.values,  # Original indices\n                chunk_start=i,\n                chunk_end=min(i + chunk_size, len(df))\n            )\n            del training_embeddings\n            \n            #process label encoding\n            train_labels_encodings = torch.tensor(encode_tags(training_labels, padding_value = padding_value))\n            train_tensor_enc = F.one_hot(train_labels_encodings.clamp(min=0), num_classes=9)\n            # Replace one-hot vectors for padding values with all zeros\n            # This creates a mask where True indicates padding positions\n            padding_mask = (train_labels_encodings == padding_value).unsqueeze(-1)\n            # Zero out the one-hot vectors at padding positions\n            train_tensor_enc = train_tensor_enc.masked_fill(padding_mask, 0)\n            print(f\"Final shape: {train_tensor_enc.shape}\") \n\n            del training_labels\n\n            # Save to NPZ\n            npz_path = os.path.join(output_dir, f'{label}_labels_chunk_{chunk_num:04d}.npz')\n            \n            # Save embeddings and any metadata\n            np.savez_compressed(\n                npz_path,\n                embeddings=tensor_to_numpy(train_tensor_enc),\n                # indices=chunk.index.values,  # Original indices\n                chunk_start=i,\n                chunk_end=min(i + chunk_size, len(df))\n            )\n            # Clear memory\n            del train_labels_encodings, train_tensor_enc, padding_mask\n            \n            # Update progress\n            pbar.update(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:08:04.177894Z","iopub.execute_input":"2025-06-30T06:08:04.178473Z","iopub.status.idle":"2025-06-30T06:08:04.190292Z","shell.execute_reply.started":"2025-06-30T06:08:04.178450Z","shell.execute_reply":"2025-06-30T06:08:04.189399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tag2id = {'S': 0, 'H': 1, 'B': 2, 'I': 3, 'L': 4, 'P': 5, 'G': 6, 'E': 7, 'T': 8}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T04:50:31.232168Z","iopub.execute_input":"2025-06-30T04:50:31.232409Z","iopub.status.idle":"2025-06-30T04:50:31.247827Z","shell.execute_reply.started":"2025-06-30T04:50:31.232386Z","shell.execute_reply":"2025-06-30T04:50:31.247323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_dfs = ['train', 'val', 'test']\n# label_dfs = ['val']\nfor i in range(len(dfs)):\n    memory_efficient_processing(model, dfs[i], tag2id, label = label_dfs[i], chunk_size=5000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:08:10.881295Z","iopub.execute_input":"2025-06-30T06:08:10.881568Z","iopub.status.idle":"2025-06-30T06:08:18.422216Z","shell.execute_reply.started":"2025-06-30T06:08:10.881549Z","shell.execute_reply":"2025-06-30T06:08:18.421295Z"}},"outputs":[],"execution_count":null}]}